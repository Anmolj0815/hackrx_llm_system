# -*- coding: utf-8 -*-
"""document_parser

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1uzJea2lQfOsFqNlP7QpZiF_U6hn2n15U
"""

import io
import requests
from pypdf import PdfReader
# from docx import Document # Uncomment if you need to parse .docx files
# import email # Uncomment if you need to parse .eml files

def download_file(url: str) -> io.BytesIO:
    """Downloads a file from a given URL and returns it as a BytesIO object."""
    try:
        response = requests.get(url, timeout=10) # Added timeout for robustness
        response.raise_for_status()  # Raise an HTTPError for bad responses (4xx or 5xx)
        return io.BytesIO(response.content)
    except requests.exceptions.Timeout:
        print(f"Error: Download timed out for {url}")
        raise ValueError(f"File download timed out for {url}")
    except requests.exceptions.RequestException as e:
        print(f"Error downloading file from {url}: {e}")
        raise ValueError(f"Failed to download file from {url}: {e}")

def extract_text_from_pdf(file_stream: io.BytesIO) -> str:
    """
    Extracts text from a PDF file stream using pypdf.
    Includes a basic check for low text density, hinting at potential OCR need.
    """
    text = ""
    reader = None
    try:
        reader = PdfReader(file_stream)
        num_pages = len(reader.pages)
        if num_pages == 0:
            return "" # No pages in PDF

        extracted_chars = 0
        for page_num, page in enumerate(reader.pages):
            page_text = page.extract_text()
            if page_text:
                text += page_text + "\n"
                extracted_chars += len(page_text.strip())

        # Heuristic to detect if it's likely an image-only PDF:
        # If very few characters extracted per page on average.
        avg_chars_per_page = extracted_chars / num_pages if num_pages > 0 else 0

        # This threshold might need tuning based on actual document types
        if avg_chars_per_page < 50 and extracted_chars < 200:
            print(f"Warning: Low text density detected in PDF ({avg_chars_per_page:.2f} chars/page). "
                  "This might be an image-only PDF requiring OCR for full text extraction.")
            # In a real scenario, you'd call an OCR function here, e.g.:
            # file_stream.seek(0) # Reset stream for OCR
            # return call_ocr_service(file_stream)
            pass # For now, we proceed with whatever pypdf extracted or empty string

    except Exception as e:
        print(f"Error extracting text from PDF with pypdf: {e}")
        # Potentially try OCR here as a fallback
        # file_stream.seek(0)
        # return call_ocr_service(file_stream) # Placeholder
        pass

    return text.strip()

# Placeholder for OCR service integration (e.g., Google Cloud Vision, AWS Textract)
# def call_ocr_service(file_stream: io.BytesIO) -> str:
#     """
#     INTEGRATE YOUR CHOSEN OCR SERVICE HERE.
#     Example using Google Cloud Vision (requires 'google-cloud-vision' pip package and credentials):
#     from google.cloud import vision
#     client = vision.ImageAnnotatorClient()
#     image = vision.Image(content=file_stream.read())
#     response = client.document_text_detection(image=image)
#     return response.full_text_annotation.text
#
#     For now, it's just a placeholder:
#     """
#     print("OCR service placeholder called. Full OCR integration is pending.")
#     return "" # Return empty string or error message for now

def parse_document(url: str) -> str:
    """
    Parses a document from a URL and extracts its text content.
    Supports PDF. Extend this for .docx, .eml.
    """
    file_stream = None
    try:
        file_stream = download_file(url)

        # Simple MIME type or extension check
        if "pdf" in url.lower():
            return extract_text_from_pdf(file_stream)
        # elif "docx" in url.lower():
        #     doc = Document(file_stream)
        #     return "\n".join([para.text for para in doc.paragraphs])
        # elif "eml" in url.lower():
        #     msg = email.message_from_bytes(file_stream.read())
        #     # Extract plain text payload; handle multipart if needed
        #     if msg.is_multipart():
        #         for part in msg.walk():
        #             ctype = part.get_content_type()
        #             cdispo = str(part.get('Content-Disposition')) # check filename
        #             if ctype == 'text/plain' and 'attachment' not in cdispo:
        #                 return part.get_payload(decode=True).decode()
        #     else:
        #         return msg.get_payload(decode=True).decode()
        else:
            print(f"Warning: Attempting generic text extraction for unsupported type: {url}")
            # Fallback for unknown types: try to decode as plain text
            return file_stream.read().decode('utf-8', errors='ignore').strip()
    except Exception as e:
        print(f"Failed to parse document from {url}: {e}")
        raise ValueError(f"Could not process document from {url}: {e}")

def chunk_text(text: str, chunk_size: int = 1000, chunk_overlap: int = 200) -> list[str]:
    """Splits text into overlapping chunks. Handles empty or short texts gracefully."""
    if not text or len(text.strip()) == 0:
        return []

    text_len = len(text)
    if text_len <= chunk_size:
        return [text]

    chunks = []
    current_start = 0
    while current_start < text_len:
        chunk_end = min(current_start + chunk_size, text_len)
        chunk = text[current_start:chunk_end]
        chunks.append(chunk)

        if chunk_end == text_len:
            break # Reached end of text

        # Move to the next chunk start, considering overlap
        current_start += (chunk_size - chunk_overlap)
        # Ensure current_start doesn't skip too far if chunk_size - chunk_overlap is very small
        if current_start >= text_len:
            break # Avoid infinite loop if overlap is too large relative to chunk_size

    return chunks



